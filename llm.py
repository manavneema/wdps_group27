# llm_interface.py
import logging
from llama_cpp import Llama

logger = logging.getLogger(__name__)

class LLMInterface:
    def __init__(self, model_path):
        """
        Initializes the LLM interface with the specified model.

        Parameters:
            model_path (str): Path to the LLM model file.
        """
        self.llm = Llama(model_path=model_path, verbose=False)
        logging.info(f"LLM model loaded from {model_path}")

    def get_response(self, prompt, max_tokens=32, temperature=0.0, top_p=0.0, seed=42):
        """
        Generates a response from the LLM based on the given prompt.

        Parameters:
            prompt (str): The input prompt/question.
            max_tokens (int): Maximum number of tokens to generate.
            temperature (float): Sampling temperature.
            top_p (float): Nucleus sampling probability.
            seed (int): Random seed for reproducibility.

        Returns:
            str: The generated response text.
        """
        try:
            output = self.llm(
                prompt,
                max_tokens=max_tokens,
                echo=False,
                seed=seed,
                temperature=temperature,
                top_p=top_p
            )
            logging.info(f"LLM response for prompt '{prompt}': {output['choices']}")
            if not output['choices']:
                logging.warning("No output generated by the LLM.")
                return ""
            llm_output_text = output['choices'][0]['text'].strip()
            if not llm_output_text:
                logging.warning("LLM did not generate any response.")
            return llm_output_text
        except Exception as e:
            logging.error(f"Error getting response from LLM: {e}")
            return ""